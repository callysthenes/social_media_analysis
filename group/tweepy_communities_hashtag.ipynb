{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Academic Account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successful Authentication\n"
     ]
    }
   ],
   "source": [
    "# API keys corresponding to an academic account\n",
    "api_key = \"4OxfMXFZL9yBDYboGSfcjbFsc\"\n",
    "api_secrets = \"Ny9LS2gPD7plQy48c3EpQ3UDvVfqrwvJyZLrAJlgEhQtWz3krW\"\n",
    "access_token =  \"1581220096529375238-2Kt6eULOqBD7EvtpqTf4gvU7n69MZ3\"\n",
    "access_secret = \"7irLEy7IiGfW72gsh895wntdk3n94upoyXtUMGSo98mch\"\n",
    "bearer_token = \"AAAAAAAAAAAAAAAAAAAAAPEbiQEAAAAAuuFvB9xcB%2FR4J2RXWNApbAWQ3PY%3DVqvJvvr5WlYDYSfMq5GcZqGGu2gapcfF0ezvJzCWfDaodqSE4L\"\n",
    "# Authenticate to Twitter\n",
    "auth = tweepy.OAuthHandler(api_key,api_secrets)\n",
    "auth.set_access_token(access_token,access_secret)\n",
    " \n",
    "api = tweepy.API(auth)\n",
    "try:\n",
    "    api.verify_credentials()\n",
    "    print('Successful Authentication')\n",
    "    #commented so it doesn't appear in hdfs\n",
    "except:\n",
    "    print('Failed authentication')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = tweepy.Client(bearer_token='AAAAAAAAAAAAAAAAAAAAAPEbiQEAAAAAuuFvB9xcB%2FR4J2RXWNApbAWQ3PY%3DVqvJvvr5WlYDYSfMq5GcZqGGu2gapcfF0ezvJzCWfDaodqSE4L')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Searching Twitter\n",
    "\n",
    "Search for whatever trait your community has in common. In this case, I searched for a popular Twitter hashtag, limiting the results to the 4000 most recent Tweets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag = api.search_tweets(q='#yuan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT @KitcoNewsNOW: Ex-Goldman chief economist calls on BRICS to challenge USD's dominance as China leads de-dollarization trend https://t.co…\n",
      "FIRSTPOST\n",
      "\n",
      "Saudi Arabia Enters China-Led SCO: US' Hegemony in West Asia Over?\n",
      "\n",
      "https://t.co/7INThitz9d\n",
      "\n",
      "#USDollar… https://t.co/JgApVFIDp7\n",
      "Eğer Çin/Rusya'nın #yuan adımına seviyorsanız bir söz ileteyim size;\n",
      "\n",
      "Amaç #ABD'yi bitirmek ise, Tüm dünya ABD'ye a… https://t.co/cuhyuUlWdH\n",
      "RT @andrewliberty23: #China settles first LNG trade in #yuan.\n",
      "\n",
      "#Brazil-China trade agreement substitutes dollars for yuan and reals.\n",
      "\n",
      "My fu…\n",
      "RT @KitcoNewsNOW: Ex-Goldman chief economist calls on BRICS to challenge USD's dominance as China leads de-dollarization trend https://t.co…\n",
      "Ex-Goldman chief economist calls on BRICS to challenge USD's dominance as China leads de-dollarization trend… https://t.co/w2niaaNBeu\n",
      "RT @BPartisans: BOUM !!!!!\n",
      "La France signe un accord non libellé en USD sur le commerce du GNL.\n",
      "RIP dollar américain.\n",
      "\n",
      "#China et #France To…\n",
      "RT @andrewliberty23: #China settles first LNG trade in #yuan.\n",
      "\n",
      "#Brazil-China trade agreement substitutes dollars for yuan and reals.\n",
      "\n",
      "My fu…\n",
      "RT @EmilieDefresne: Velléités d'#indépendance de #Total?\n",
      "Contournement des #sanctions #antirusses?\n",
      "#CNOOC, société chinoise et #TotalEnergi…\n",
      "Los chinos sacando pecho , imagen de global times\n",
      "#Yuan https://t.co/HSJWaIgWUV\n",
      "RT @negocios_tv: ÚLTIMA HORA: China y Brasil trasladan el comercio mutuo al Yuan dejando al dólar fuera del mapa\n",
      "\n",
      "https://t.co/grENhAuwWr…\n",
      "RT @negocios_tv: ÚLTIMA HORA: China y Brasil trasladan el comercio mutuo al Yuan dejando al dólar fuera del mapa\n",
      "\n",
      "https://t.co/grENhAuwWr…\n",
      "RT @BPartisans: BOUM !!!!!\n",
      "La France signe un accord non libellé en USD sur le commerce du GNL.\n",
      "RIP dollar américain.\n",
      "\n",
      "#China et #France To…\n",
      "RT @negocios_tv: ÚLTIMA HORA: China y Brasil trasladan el comercio mutuo al Yuan dejando al dólar fuera del mapa\n",
      "\n",
      "https://t.co/grENhAuwWr…\n",
      "@MasonVersluis #China seems to be doing fine with these practices. Look at the recent success of the #Chinese #yuan.\n"
     ]
    }
   ],
   "source": [
    "#show contents of search\n",
    "for tweet in hashtag:\n",
    "    print(tweet.text)\n",
    "      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lists for each field desired from the tweets.\n",
    "sn = []\n",
    "text = []\n",
    "timestamp =[]\n",
    "for tweet in hashtag:\n",
    "    #print (tweet.user.screen_name, tweet.created_at, tweet.text)\n",
    "    timestamp.append(tweet.created_at)\n",
    "    sn.append(tweet.user.screen_name)\n",
    "    text.append(tweet.text)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert lists to dataframe\n",
    "df = pd.DataFrame()\n",
    "df['timestamp'] = timestamp\n",
    "df['sn'] = sn\n",
    "df['text'] = text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare for date filtering. Adding an EST time column since chat hosted by people in that time zone.\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "df['EST'] = df['timestamp'] - pd.Timedelta(hours=5) #Convert to EST\n",
    "\n",
    "df['EST'] = pd.to_datetime(df['EST'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>sn</th>\n",
       "      <th>text</th>\n",
       "      <th>EST</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-03-30 17:38:45+00:00</td>\n",
       "      <td>EllieLove2019</td>\n",
       "      <td>RT @KitcoNewsNOW: Ex-Goldman chief economist c...</td>\n",
       "      <td>2023-03-30 12:38:45+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-03-30 17:37:20+00:00</td>\n",
       "      <td>THEENDGAMEWAR1</td>\n",
       "      <td>FIRSTPOST\\n\\nSaudi Arabia Enters China-Led SCO...</td>\n",
       "      <td>2023-03-30 12:37:20+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-03-30 17:36:58+00:00</td>\n",
       "      <td>FatihthePau</td>\n",
       "      <td>Eğer Çin/Rusya'nın #yuan adımına seviyorsanız ...</td>\n",
       "      <td>2023-03-30 12:36:58+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-03-30 17:36:58+00:00</td>\n",
       "      <td>dizzy_lizzyy1</td>\n",
       "      <td>RT @andrewliberty23: #China settles first LNG ...</td>\n",
       "      <td>2023-03-30 12:36:58+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-03-30 17:35:48+00:00</td>\n",
       "      <td>CDeutschmanek</td>\n",
       "      <td>RT @KitcoNewsNOW: Ex-Goldman chief economist c...</td>\n",
       "      <td>2023-03-30 12:35:48+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  timestamp              sn  \\\n",
       "0 2023-03-30 17:38:45+00:00   EllieLove2019   \n",
       "1 2023-03-30 17:37:20+00:00  THEENDGAMEWAR1   \n",
       "2 2023-03-30 17:36:58+00:00     FatihthePau   \n",
       "3 2023-03-30 17:36:58+00:00   dizzy_lizzyy1   \n",
       "4 2023-03-30 17:35:48+00:00   CDeutschmanek   \n",
       "\n",
       "                                                text                       EST  \n",
       "0  RT @KitcoNewsNOW: Ex-Goldman chief economist c... 2023-03-30 12:38:45+00:00  \n",
       "1  FIRSTPOST\\n\\nSaudi Arabia Enters China-Led SCO... 2023-03-30 12:37:20+00:00  \n",
       "2  Eğer Çin/Rusya'nın #yuan adımına seviyorsanız ... 2023-03-30 12:36:58+00:00  \n",
       "3  RT @andrewliberty23: #China settles first LNG ... 2023-03-30 12:36:58+00:00  \n",
       "4  RT @KitcoNewsNOW: Ex-Goldman chief economist c... 2023-03-30 12:35:48+00:00  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write out Tweets in case they are needed later.\n",
    "df.to_csv('yuantweets.csv',index = False,encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a list of the unique usernames in order to see which users we need to retrieve friends for.\n",
    "allNames = list(df['sn'].unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Retrieve User Information\n",
    "\n",
    "Now having a list of all of the users we would like to include in our community, we can retrive additional necessary information such as who their are following in order to build our social graph.\n",
    "\n",
    "Note that Twitter does have strict rate limits that can cause problems at this point. A timeout is built into every iteration to minimize this. If problems still occur, you may want to include an intermediate write out on every iteration to maintain what has been captured to that point. The loop can be restarted from the nth value of allNames where a break occurs (i.e. \"for name in allNames[n:]\").\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize dataframe of users that will hold the edge relationships\n",
    "dfUsers = pd.DataFrame()\n",
    "dfUsers['userFromName'] =[]\n",
    "dfUsers['userFromId'] =[]\n",
    "dfUsers['userToId'] = []\n",
    "count = 0 \n",
    "\n",
    "nameCount = len(allNames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Twitter call 13.3% complete."
     ]
    }
   ],
   "source": [
    "\n",
    "# The choice to retrieve friends (who the user is following) rather than followers is intentional.\n",
    "# Either would work. However, many Twitter users follow fewer users than are following them, especially the most popular accounts. \n",
    "# This reduces the number of very large calls to Twitter API, which seemed to cause problems.\n",
    "for name in allNames:\n",
    "    # Build list of friends    \n",
    "    currentFriends = []\n",
    "    for page in tweepy.Cursor(api.get_friends, screen_name=name).pages(2):\n",
    "        currentFriends.extend(page)\n",
    "    currentId = api.get_user(screen_name=name).id\n",
    "    currentId = [currentId] * len(currentFriends)\n",
    "    currentName = [name] * len(currentFriends)   \n",
    "    dfTemp = pd.DataFrame()\n",
    "    dfTemp['userFromName'] = currentName\n",
    "    dfTemp['userFromId'] = currentId\n",
    "    dfTemp['userToId'] = currentFriends\n",
    "    dfUsers = pd.concat([dfUsers,dfTemp])\n",
    "    time.sleep(70) # avoids hitting Twitter rate limit\n",
    "    # Progress bar to track approximate progress\n",
    "    count +=1\n",
    "    per = round(count*100.0/nameCount,1)\n",
    "    sys.stdout.write(\"\\rTwitter call %s%% complete.\" % per)\n",
    "    sys.stdout.flush()    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again, to limit the number of calls to Twitter API, just do lookups on followers that connect to those in our user group.\n",
    "# We are not interested in \"friends\" that are not part of this community.\n",
    "fromId = dfUsers['userFromId'].unique()\n",
    "dfChat = dfUsers[dfUsers['userToId'].apply(lambda x: x in fromId)]\n",
    "\n",
    "# No more Twitter API lookups are necessary. Create a lookup table that we will use to get the verify the userToName\n",
    "dfLookup = dfChat[['userFromName','userFromId']]\n",
    "dfLookup = dfLookup.drop_duplicates()\n",
    "dfLookup.columns = ['userToName','userToId']\n",
    "dfCommunity = dfUsers.merge(dfLookup, on='userToId')\n",
    "\n",
    "dfCommunity.to_csv('dfCommunity.csv',index = False,encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['userFromName', 'userFromId'], dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)\n",
      "Cell \u001b[1;32mIn[63], line 7\u001b[0m\n",
      "\u001b[0;32m      4\u001b[0m dfChat \u001b[39m=\u001b[39m dfUsers[dfUsers[\u001b[39m'\u001b[39m\u001b[39muserToId\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: x \u001b[39min\u001b[39;00m fromId)]\n",
      "\u001b[0;32m      6\u001b[0m \u001b[39m# No more Twitter API lookups are necessary. Create a lookup table that we will use to get the verify the userToName\u001b[39;00m\n",
      "\u001b[1;32m----> 7\u001b[0m dfLookup \u001b[39m=\u001b[39m dfChat[[\u001b[39m'\u001b[39;49m\u001b[39muserFromName\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39muserFromId\u001b[39;49m\u001b[39m'\u001b[39;49m]]\n",
      "\u001b[0;32m      8\u001b[0m dfLookup \u001b[39m=\u001b[39m dfLookup\u001b[39m.\u001b[39mdrop_duplicates()\n",
      "\u001b[0;32m      9\u001b[0m dfLookup\u001b[39m.\u001b[39mcolumns \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39muserToName\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39muserToId\u001b[39m\u001b[39m'\u001b[39m]\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\pedro\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3813\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n",
      "\u001b[0;32m   3811\u001b[0m     \u001b[39mif\u001b[39;00m is_iterator(key):\n",
      "\u001b[0;32m   3812\u001b[0m         key \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(key)\n",
      "\u001b[1;32m-> 3813\u001b[0m     indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49m_get_indexer_strict(key, \u001b[39m\"\u001b[39;49m\u001b[39mcolumns\u001b[39;49m\u001b[39m\"\u001b[39;49m)[\u001b[39m1\u001b[39m]\n",
      "\u001b[0;32m   3815\u001b[0m \u001b[39m# take() does not accept boolean indexers\u001b[39;00m\n",
      "\u001b[0;32m   3816\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(indexer, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39m==\u001b[39m \u001b[39mbool\u001b[39m:\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\pedro\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6070\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n",
      "\u001b[0;32m   6067\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;32m   6068\u001b[0m     keyarr, indexer, new_indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reindex_non_unique(keyarr)\n",
      "\u001b[1;32m-> 6070\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_raise_if_missing(keyarr, indexer, axis_name)\n",
      "\u001b[0;32m   6072\u001b[0m keyarr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtake(indexer)\n",
      "\u001b[0;32m   6073\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, Index):\n",
      "\u001b[0;32m   6074\u001b[0m     \u001b[39m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\pedro\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6130\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n",
      "\u001b[0;32m   6128\u001b[0m     \u001b[39mif\u001b[39;00m use_interval_msg:\n",
      "\u001b[0;32m   6129\u001b[0m         key \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(key)\n",
      "\u001b[1;32m-> 6130\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNone of [\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m] are in the [\u001b[39m\u001b[39m{\u001b[39;00maxis_name\u001b[39m}\u001b[39;00m\u001b[39m]\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;32m   6132\u001b[0m not_found \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[39m.\u001b[39mnonzero()[\u001b[39m0\u001b[39m]]\u001b[39m.\u001b[39munique())\n",
      "\u001b[0;32m   6133\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mnot_found\u001b[39m}\u001b[39;00m\u001b[39m not in index\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\n",
      "\u001b[1;31mKeyError\u001b[0m: \"None of [Index(['userFromName', 'userFromId'], dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
